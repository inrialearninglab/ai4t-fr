---
title:  L’IA peut-elle être sexiste ?
description:
hide:
- toc
---


<center><img src="../Images/ondira-levrres-intelligence-artificielle-femmes.jpg" alt="Photo by Fernando Arcos from Pexels" width="500"></center>
<center>[Photo: CBC/Radio-Canada -iStock - Editing: Amarilys Proulx]()</center>

Une intelligence artificielle ne pense pas. Alors, peut-elle être raciste, sexiste ou homophobe ? C’est possible qu’elle reproduise des mécanismes discriminants, si le mécanisme a été entraîné par des données biaisées. L’être humain a dans son raisonnement des _biais cognitifs_.

Les **Biais cognitifs** sont l’ensemble des facteurs psychiques, moraux, sociaux, culturels… qui influencent, sans que l’on s’en rende compte, nos mécanismes de pensées. Ces biais cognitifs ont une incidence sur la pensée logique, car ils influencent nos modes de réflexion et nous amènent à prendre des décisions en étant guidé par des facteurs qui ne sont pas rationnels. Ils peuvent être alors à l'origine de décisions discriminantes.

Or c’est nous qui fournissons des données aux algorithmes. Si les données qu’on fournit comportent des biais, cela se retrouvera forcément dans les résultats. Par exemple, l’entreprise Amazon a des besoins de recrutement énormes, et pour en faciliter la gestion, a décidé d’utiliser une intelligence artificielle capable de trier les CV. Pour que cette IA apprenne à trier les profils, Amazon a fourni l’ensemble des données de recrutement qu’elle avait enregistré entre 2004 et 2014. Cependant, au bout d’un an, Amazon s’est aperçu que l’IA rejetait systématiquement les profils de femmes pour les postes techniques et informatiques. L’intelligence artificielle, avait simplement déterminé en analysant les données qui lui avaient été fournies, que les recrutements effectués ces dix dernières années pour ces postes étaient très largement masculins<sup>1</sup>. Amazon a ainsi dû arrêter cette intelligence artificielle, parce que les résultats obtenus avaient des biais sexistes. Or ce ne sont pas que des biais sur les données, il y aussi des biais liés à la façon dont les algorithmes eux-mêmes sont mis en oeuvre.

Les technologies ne sont donc pas neutres, elles sont indirectement le reflet des modes de fonctionnement de leurs concepteurs et de nos sociétés. Même des facteurs comme la langue peuvent avoir une influence sur elles. En France, la composition de la langue favorise, davantage que dans d’autres pays, un apprentissage genré. Les noms étant très majoritairement masculins, un algorithme connaît plus de noms masculins que féminins. Ainsi, des sites de traduction, comme “Google traduction” ont tendance à donner une traduction masculine à des noms qui sont neutres dans une autre langue<sup>2</sup>.

L’intelligence artificielle peut renforcer les inégalités, comme les combattre. Face à ces enjeux éthiques et techniques, plusieurs modes d’actions sont envisagés. Par exemple, garantir plus de mixité dans les sciences informatiques, pour que les hommes n’y soient plus surreprésentés. Ou encore, réussir à vaincre les biais cognitifs en apprenant à l’algorithme à les reconnaître. Cette seconde option demande aux concepteurs d’envisager en amont les biais cognitifs à combattre, alors même qu’ils n’en ont pas toujours conscience. La meilleure solution reste sûrement de garantir la diversité dans les données d’apprentissage, les données étant peut-être plus faciles à changer que les mentalités<sup>3</sup>.

* * *

<sup>1</sup> _[Amazon a dû se débarrasser d’une intelligence artificielle sexiste”,](http://www.slate.fr/story/168413/amazon-abandonne-intelligence-artificielle-sexiste) Slatefr,10 octobre 2018. ; [Amazon met fin à une intelligence artificielle sexiste”](https://www.franceinter.fr/emissions/c-est-deja-demain/c-est-deja-demain-12-octobre-2018) par Hélène Chevallier, France Inter, emission c'est déjà demain, vendredi 12 octobre 2018 ; [Amazon a du débrancher un logiciel de recrutement qui s’est révélé sexiste”](https://www.europe1.fr/emissions/axel-de-tarle-vous-parle-economie/amazon-a-du-debrancher-une-logiciel-de-recrutement-qui-sest-revele-sexiste-37768493), La matinale d'Europe 1, le 6h - 9h, l'édito économique d'Axel de Tarlé, le 12 octobre 2018._  
<sup>2</sup> _[Quand l’Intelligence Artificielle rencontre les Sciences du Langage”](https://chut.media/portraits/intelligence-artificielle-sciences-du-langage/) par Aurore Bisicchia, Chut!, janvier 2005._  
<sup>3</sup> _[“L’IA est–elle sexiste, elle aussi ?”](https://www.lemonde.fr/blog/binaire/2018/03/08/lia-est-elle-sexiste-elle-aussi/) par Anne-Marie Kermarrec, Blog Binaire, Le Monde, 08 mars 2018._  
_["Les biais sexistes de l'IA peuvent être corrigés, selon les chercheuses Aude Bernheim et Flora Vincent"](https://www.usinenouvelle.com/editorial/les-biais-sexistes-de-l-ia-peuvent-etre-corriges-selon-les-chercheuses-aude-bernheim-et-flora-vincent.N815345), L'Usine Nouvelle, propos recueillis par Marion Garreau, 08/03/2019._  
_["Le secteur de l’intelligence artificielle est aussi masculin qu’un bar des sports le soir d’un match de Ligue 1”](https://www.lopinion.fr/edition/politique/secteur-l-intelligence-artificielle-est-aussi-masculin-qu-bar-sports-180114), Interview d'Aude Bernheim et Flora Vincent par Irène Inchauspé, L’opinion, 08 Mars 2019._
