---
title: Une éthique de l’IA « au futur »
description:
hide:
- toc
---

_Article publié sur le blob Binaire [(www.lemonde.fr)](https://www.lemonde.fr/blog/binaire/2020/03/25/une-ethique-de-lia-au-futur/)_

L’Intelligence Artificielle (IA) est au cœur de nombreux débats et polémiques dans la société. Nul domaine de la vie sociale et économique ne semble épargné par le sujet. Ce qui est intéressant à souligner, c’est que dans tous les débats aujourd’hui autour de l’IA, l’éthique est à chaque fois convoquée. Personne ne semble réduire la question de l’IA à une simple problématique technique. En atteste actuellement la prolifération des rapports sur l’éthique de l’IA, produits tant par des entreprises privées, des acteurs publics ou des organisations de la société civile.

Parmi les rapports, les propos peuvent diverger quelque peu, mais ils semblent tous souscrire à une même forme d’impératif éthique. Il s’agit de l’injonction à anticiper les impacts des technologies. L’éthique – pour reprendre des termes du philosophe Hans Jonas – doit se faire aujourd’hui «éthique du futur». Anticiper les impacts du développement de l’IA devient donc un impératif éthique.

Un tel impératif n’est pas neuf. Il est notamment au cœur de la démarche de la « recherche et l’innovation responsable » prôné par la Commission Européenne. Il me semble qu’un tel concept analogue de responsabilité est également au cœur des récents rapports français sur l’IA. Je me centrerai ici sur trois rapports récents : celui de la CNIL <sup>1</sup>, le rapport Villani <sup>2</sup>, ainsi que le rapport de la CERNA <sup>3</sup>. Commençons par le rapport Villani. Ce rapport précise que : « la loi ne peut pas tout, entre autres car le temps du droit est bien plus long que celui du code. Il est donc essentiel que les « architectes » de la société numérique (…) prennent leur juste part dans cette mission en agissant de manière responsable. Cela implique qu’ils soient pleinement conscients des possibles effets négatifs de leurs technologies sur la société et qu’ils œuvrent activement à les limiter ».

Le mouvement est double : anticiper l’aval du développement technologique, pour – en amont – modifier la conception afin d’empêcher les impacts éthiques négatifs. A l’appui de cette démarche, le rapport Villani envisage d’obliger les développeurs d’IA à réaliser une évaluation de l’impact des discriminations (discrimination impact assessment ) afin de « les obliger à se poser les bonnes questions au bon moment ». On retrouve également dans le rapport de la CNIL, une recommandation similaire : « Travailler le design des systèmes algorithmiques au service de la liberté humaine ». Anticiper pour intégrer l’éthique au plus tôt du développement technologique. La volonté de mettre en œuvre une « éthique au futur » est tout à fait louable mais de nombreuses questions restent en suspens que je voudrais passer en revue.

**Le futur est-il anticipable?**  

On peut tout d’abord poser une critique d’ordre épistémologique. Comment juger éthiquement des potentialités ouvertes par le numérique ? Face à un tel développement, ne sommes-nous pas plongés dans ce que les économistes appellent une « incertitude radicale » ? Les possibles ouverts par les « Big Data » sont un bon exemple de cette incertitude radicale. Sous cette appellation, il est fait référence au phénomène de prolifération ininterrompue et exponentielle des données. Cette évolution a fait évoluer le langage technique utilisé pour mesurer la puissance de stockage des données. De l’octet, nous sommes passés au mégaoctet, au gigaoctet, etc. Comme le souligne Eric Sardin, avec des unités de mesures comme le petaoctet, le zetaoctet ou le yotaoctet, il est clair que nous manions des unités de mesure qui excèdent purement et simplement nos structures humaines d’intelligibilité <sup>4</sup>. De plus, les rapports n’insistent-ils pas tous sur le caractère imprévisible de certains algorithmes d’apprentissage ? Ayant conscience de cette imprévisibilité, la CNIL défend, dans son rapport un principe de « vigilance » qui institue l’obligation d’une réflexion éthique continue. Mais plus fondamentalement, ne faut-il pas reconnaître que tout n’est pas anticipable ?
<center><img src="https://storage.googleapis.com/prd-blogs/2020/03/c1d499a6-bytes.jpg" alt="Photo by Ian-S from Visualhunt.com - CC BY-NC-ND" width="500"></center>
<center>[Ian-S](https://visualhunt.co/a4/8129d567) on [Visualhunt.com](https://visualhunt.com/re6/e9ca0ca4) / [CC BY-NC-ND](http://creativecommons.org/licenses/by-nc-nd/2.0/) </center>

**Un futur « colonisé »?**

Il est également important de prendre conscience que ce futur que l’on nous enjoint d’anticiper est saturé de peurs, d’attentes et de promesses. Le futur n’est pas un temps vide, mais un temps construit par des scénarios, road-maps et discours prophétiques. Pour le dire en reprenant une expression de Didier Bigo, le futur est « colonisé » par de nombreux acteurs qui essaient d’imposer leur vision comme une matrice commune de toute anticipation du futur. Initialement, il a forgé cette expression dans le cadre d’une réflexion sur les technologies de surveillance, pour désigner les prétentions et stratégies des experts qui appréhendent le futur comme un « futur antérieur, comme un futur déjà fixé, un futur dont ils connaissent les événements » <sup>5</sup>.

L’éthique robotique nous fait courir le même risque d’une colonisation du futur comme l’illustre Paul Dumouchel et Luisa Damiano dans leur livre _Vivre avec des robots_. Ces derniers pensent notamment à des auteurs comme Wallach et Allen, dans _Moral Machines, Teaching Robots Right from Wrong_, qui proposent un programme visant à enseigner aux robots la différence entre le bien et le mal, à en faire des « agents moraux artificiels ». Mais « programmer » un agent moralement autonome n’est-il pas une contradiction dans les termes ? Mon propos n’est pas de rentrer ici dans un débat métaphysique à ce sujet. Je voudrais plutôt souligner le fait qu’un tel programme a littéralement « colonisé » l’horizon d’attente des débats sur l’éthique robotique. Suivons Dumouchel et Damiano pour saisir ce point. Ils relèvent que, de l’avis même de certains des protagonistes de l’éthique robotique, « nous sommes encore loin de pouvoir créer des agents artificiels autonomes susceptibles d’être de véritables agents moraux. Selon eux, nous ne savons même pas si nous en serons capable un jour. Nous ne savons pas si de telles machines sont possibles » <sup>6</sup>. Une des réponses avancées par les tenants de l’éthique robotique est alors qu’« il ne faut pas attendre pour élaborer de telles règles que nous soyons pris au dépourvu par l’irruption soudaine d’agents artificiels autonomes. Il est important déjà à se préparer à un avenir inévitable. Les philosophes doivent dès aujourd’hui participer au développement des robots qui demain peupleront notre quotidien en mettant au point des stratégies qui permettent d’inscrire dans les robots des règles morales qui contraignent leur comportements »<sup>7</sup>.

Ce thème de l’autonomisation _inévitable_ des machines est puissant et tout à fait problématique. Parmi tous les possibles, l’attention se trouve focalisée sur ce scénario posé comme « inéluctable ». Une telle focalisation de l’attention pose plusieurs problèmes. D’une part, elle pose des questions épistémologiques : d’où vient que l’on puisse soutenir cette inéluctabilité ? D’autre part, pour Dumouchel et Damiano, cette croyance, bien que non rationnelle, a des effets réels certains : elle détourne l’attention d’enjeux de pouvoir qui se posent dès à présent, à savoir que l’autonomisation des robots, le fait de leur déléguer des décisions et de leur laisser choisir par eux-mêmes signifie peut-être la perte du pouvoir de décision de quelques-uns, mais intensifie aussi la concentration de la décision dans les mains de certains (les programmateurs, les propriétaires des robots, etc.).

<center><img src="https://storage.googleapis.com/prd-blogs/2020/03/7b8b81cc-forbiden-planet.jpg" alt="Photo on data representation" width="500"></center>
<center>Image extraite du film  Forbiden Planet - La planète interdite (USA, 1956) – MGM productions</center>

**Qui anticipe ?**

Autre question connexe : qui effectuera cette anticipation des impacts éthiques ? Qui seront les auteurs des discriminations impacts ? Ne s’agit-il que des seuls chercheurs ?  Sur ce point, tous les rapports précisent bien que cet impératif éthique d’anticipation ne concerne pas que ceux-ci. Le rapport de la CERNA précise que « le chercheur doit délibérer dès la conception de son projet avec les personnes ou les groupes identifiés comme pouvant être influencés ». Pour le rapport Villani, « Il faut créer une véritable instance de débat, plurielle et ouverte sur la société, afin de déterminer démocratiquement quelle IA nous voulons pour notre société ». Quant à la CNIL, elle affirme dans son rapport que « Les systèmes algorithmiques et d’intelligence artificielle sont des objets socio-techniques complexes, modelés et manipulés par de longues et complexes chaînes d’acteurs. C’est donc tout au long de la chaîne algorithmique (du concepteur à l’utilisateur final, en passant par ceux qui entraînent les systèmes et par ceux qui les déploient) qu’il faut agir, au moyen d’une combinaison d’approches techniques et organisationnelles. Les algorithmes sont partout : ils sont donc l’affaire de tous ».

Les rapports cités portent bien une attention à la question de savoir QUI anticipe. Néanmoins, l’injonction à délibérer avec « les personnes ou les groupes identifiés comme pouvant être influencés » (CERNA) ne suffit pas. Il faut que la détermination de la liste de ses personnes concernées soit un objet même de réflexion et de recherche et non pas un prérequis de cette démarche d’anticipation. En effet, l’anticipation de ces impacts peut nous faire prendre conscience de nouvelles parties prenantes à intégrer dans la réflexion. De plus, quelle forme donner à une éthique de l’IA qui soit « l’affaire de tous » ? Comment l’instituer ?

**Une prise en compte du temps qui occulte l’espace**

Enfin, une dernière question laissée en suspens est le privilège accordé au temps par rapport à la spatialité dans la réflexion éthique. Le développement du numérique et en particulier de l’IA ont contribué à l’idée que la virtualisation des échanges réduirait les distances et l’importance des lieux. Une telle croyance est par exemple au cœur du développement de la télémédecine : un malade chronique peut être surveillé indifféremment à son domicile ou en institution, un spécialiste peut être sollicité par télé-expertise quelle que soit la distance qui le sépare de son confrère, etc. Or comme l’affirment les sociologues Alexandre Mathieu-Fritz et Gérald Gaglio, « la télémédecine ne conduit pas à une abolition des frontières et des espaces, contrairement à une vision du sens commun souvent portée implicitement par les politiques publiques »<sup>8</sup>. Pour pouvoir être effectif, les actes de télémédecine demandent un certain aménagement des espaces. Dans un travail ethnographique mené auprès de patients télé-surveillés, Nelly Oudshoorn, a démontré combien les lieux, l’espace domestique et l’espace public influencent et façonnent la manière dont les technologies sont implémentées, de même qu’à l’inverse, ces technologies transforment littéralement ces espaces. La maison devient ainsi un lieu hybride, un lieu de vie médicalisé. Cette dimension spatiale ne semble pas réellement prise en compte.

Pourtant, un auteur comme Pierre Rosanvallon, nous rappelait il y a une dizaine d’années dans son ouvrage _La légitimité démocratique_ que la légitimité de l’action publique passe de plus en plus aujourd’hui par ce qu’il appelle un « principe de proximité », une attention aux contextes locaux. En atteste aujourd’hui la promotion d’une démarche d’« expérimentation » dans le domaine de l’IA en santé : « afin de bénéficier des avancées de l’IA en médecine, il est important de faciliter les expérimentations de technologie IA en santé en temps réel et au plus près des usagers, en créant les conditions réglementaires et organisationnelles nécessaires » (Rapport Villani). De manière plus inductive et partant du terrain, il s’agirait de privilégier une nouvelle construction de l’action publique : « expérimenter l’action publique » comme nous l’y invite Clément Bertholet et Laura Létourneau<sup>9</sup>.

Si cette démarche d’expérimentation invite à prendre en compte les réalités contextuelles locales, l’objectif reste toujours la « scalabilité » (le fait de pouvoir être utilisé à différentes échelles). Je reprends ce terme à l’anthropologue Anna Lowenhaupt Tsing qui le définit comme : « la capacité de projets à s’étendre sans que le cadre de leur hypothèse ne change »<sup>10</sup>. On expérimente localement certes, mais me semble-t-il pour dégager ce qui est généralisable et opérationnel de manière indifférenciée à différentes échelles. Or, un dispositif de télésurveillance – pour prendre cet exemple – peut-il s’appliquer dans tout contexte ? Tous les lieux de vie peuvent-ils devenir des lieux de soin ?

L’éthique de l’IA prend-elle bien en compte le fait que les impacts de l’IA vont se différencier selon les lieux et les espaces ? Où doivent prendre place les « ethical impact assessment » ? Doivent-ils être centralisés ou localisés dans les lieux de conceptions des objets techniques ? Plus fondamentalement, est-il possible de déterminer ces impacts sans se déplacer ? Sur ce point, la cartographie du paysage global de l’éthique de l’IA réalisée par A. Jobin et alii<sup>11</sup> est tout à fait instructive. Ils ont réalisé une analyse comparative de 84 rapports sur l’éthique de l’IA. Il s’agit de document produits par des agences gouvernementales, des firmes privées, des organisations non lucratives ou encore des sociétés savantes. Leur analyse met en avant le fait que les rapports sont produits majoritairement aux USA (20 rapports), dans l’Union européenne (19), suivi par la Grande-Bretagne (14) et le Japon (4). Les pays africains et latino-américains ne sont pas représentés indépendamment des organisations internationales ou supra-nationales. Cette distribution géographique n’est-elle pas significative de l’occultation de cette dimension spatiale ?

**Alain Loute** (Centre d’éthique médicale, labo ETHICS EA 7446, Université Catholique de Lille)

<sup>1</sup> [Comment permettre à l’homme de garder la main](https://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_garder_la_main_web.pdf) ? Les enjeux éthiques des algorithmes et de l’intelligence artificielle, CNIL, 2017.

<sup>2</sup> [Donner un sens à l’intelligence artificielle, Pour une stratégie nationale et européenne](https://www.aiforhumanity.fr/pdfs/9782111457089_Rapport_Villani_accessible.pdf), Rapport Villani, 8 mars 2018.

<sup>3</sup> [Ethique de la recherche en apprentissage machine](http://cerna-ethics-allistene.org/digitalAssets/53/53991_cerna___thique_apprentissage.pdf), Avis de la Commission de réflexion sur l’Ethique de la Recherche en sciences et technologies du Numérique d’Allistene (CERNA), juin 2017.

<sup>4</sup> E. Sardin, La vie algorithmique, Critique de la raison numérique, Ed. L’échappée, Paris 2015, pp. 21-22.

<sup>5</sup> D. Bigo, « Sécurité maximale et prévention ? La matrice du futur antérieur et ses grilles », in B. Cassin (éd.), Derrière les grilles, Sortons du tout-évaluation, Paris, Fayard, 2014, p. 111-138, p. 126.

<sup>6</sup> P. Dumouchel et L. Damiano, Vivre avec des robots, Essai sur l’empathie artificielle, Paris, Seuil, 2016, p. 191.

<sup>7</sup> ibid., p. 191-192.

<sup>8</sup> A. Mathieu-Fritz et G. Gaglio, « À la recherche des configurations sociotechniques de la télémédecine, Revue de littérature des travaux de sciences sociales », in Réseaux, 207, 2018/1, pp. 27-63.

<sup>9</sup> C. Bertholet et L. Létourneau, Ubérisons l’Etat avant que les autres ne s’en chargent, Paris, Armand Collin, 2017, p.182.

<sup>10</sup> A. Lowenhaupt Tsing, Le champignon de la fin du monde, Sur la possibilité de vivre dans les ruines du capitalisme, Paris, La Découverte, 2017, p. 78.

<sup>11</sup> A. Jobin, M. Ienca, & E. Vayena, « The global landscape of AI ethics guidelines », in Nat Mach Intell, 1, 2019, pp. 389–399.
